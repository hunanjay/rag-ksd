"""
æ–‡ä»¶è§£ææ¨¡å— - ç”¨äº Moodle é™„ä»¶åˆ†æ

ä½¿ç”¨è½»é‡çº§åº“æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼Œæå–æ–‡æœ¬å†…å®¹ä¾› LLM åˆ†æ

ä¾èµ–:
    pip install PyPDF2 python-docx python-pptx markdown
"""

import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
import mimetypes

logger = logging.getLogger(__name__)


class FileParser:
    """æ–‡ä»¶è§£æå™¨ - ä½¿ç”¨è½»é‡çº§åº“æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼"""
    
    SUPPORTED_EXTENSIONS = {
        '.pdf', '.ppt', '.pptx', '.docx', '.md', '.txt'
    }
    
    def __init__(self):
        """åˆå§‹åŒ–æ–‡ä»¶è§£æå™¨"""
        pass  # ä¸éœ€è¦é¢„å…ˆæ£€æŸ¥ä¾èµ–ï¼ŒæŒ‰éœ€å¯¼å…¥
    
    def parse_file(
        self,
        file_path: str,
        max_chars: Optional[int] = None,
        extract_tables: bool = True,
        extract_images: bool = False
    ) -> Dict[str, Any]:
        """
        è§£ææ–‡ä»¶å¹¶æå–æ–‡æœ¬å†…å®¹
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            max_chars: æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ï¼ˆç”¨äº LLM token æ§åˆ¶ï¼‰
            extract_tables: æ˜¯å¦æå–è¡¨æ ¼å†…å®¹
            extract_images: æ˜¯å¦æå–å›¾ç‰‡ä¿¡æ¯
        
        Returns:
            è§£æç»“æœå­—å…¸ï¼ŒåŒ…å«:
            - success: æ˜¯å¦æˆåŠŸ
            - text: æå–çš„çº¯æ–‡æœ¬
            - elements: ç»“æ„åŒ–å…ƒç´ åˆ—è¡¨
            - metadata: æ–‡ä»¶å…ƒæ•°æ®
            - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        path = Path(file_path)
        
        if not path.exists():
            return {
                "success": False,
                "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}",
                "text": "",
                "elements": []
            }
        
        file_ext = path.suffix.lower()
        
        if file_ext not in self.SUPPORTED_EXTENSIONS:
            return {
                "success": False,
                "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}",
                "text": "",
                "elements": [],
                "supported_formats": list(self.SUPPORTED_EXTENSIONS)
            }
        
        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£æç­–ç•¥
            if file_ext == '.pdf':
                full_text = self._parse_pdf(path, extract_tables)
            elif file_ext in ['.ppt', '.pptx']:
                full_text = self._parse_ppt(path)
            elif file_ext in ['.doc', '.docx']:
                full_text = self._parse_docx(path)
            elif file_ext == '.md':
                full_text = self._parse_markdown(path)
            elif file_ext == '.txt':
                full_text = self._parse_text(path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
            
            original_length = len(full_text)
            
            # é™åˆ¶å­—ç¬¦æ•°
            if max_chars and len(full_text) > max_chars:
                full_text = full_text[:max_chars] + f"\n\n... (æ–‡æœ¬å·²æˆªæ–­ï¼ŒåŸå§‹é•¿åº¦: {original_length} å­—ç¬¦)"
            
            # åºåˆ—åŒ–å…ƒç´ 
            elements_list = self._serialize_elements(full_text)
            
            # æ„å»ºç»“æœ
            result = {
                "success": True,
                "text": full_text,
                "char_count": len(full_text),
                "original_char_count": original_length,
                "truncated": original_length > (max_chars or float('inf')),
                "element_count": len(elements_list),
                "file_name": path.name,
                "file_size_kb": path.stat().st_size / 1024,
                "file_type": file_ext,
                "elements": elements_list
            }
            
            logger.info(f"âœ… æˆåŠŸè§£æ {path.name}: {original_length} å­—ç¬¦")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ä»¶å¤±è´¥ {path.name}: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "text": "",
                "elements": []
            }
    
    def _parse_pdf(self, path: Path, extract_tables: bool = True) -> str:
        """è§£æ PDF æ–‡ä»¶ - ä½¿ç”¨ PyPDF2"""
        try:
            from PyPDF2 import PdfReader
        except ImportError:
            raise ImportError("è¯·å®‰è£… PyPDF2: pip install PyPDF2")
        
        logger.info(f"ğŸ“„ æ­£åœ¨è§£æ PDF: {path.name}")
        
        reader = PdfReader(str(path))
        texts = []
        
        for page_num, page in enumerate(reader.pages, 1):
            try:
                text = page.extract_text()
                if text.strip():
                    texts.append(f"=== ç¬¬ {page_num} é¡µ ===\n{text}")
            except Exception as e:
                logger.warning(f"âš ï¸  é¡µé¢ {page_num} æå–å¤±è´¥: {e}")
        
        return "\n\n".join(texts)
    
    def _parse_ppt(self, path: Path) -> str:
        """è§£æ PPT/PPTX æ–‡ä»¶ - ä½¿ç”¨ python-pptx"""
        try:
            from pptx import Presentation
        except ImportError:
            raise ImportError("è¯·å®‰è£… python-pptx: pip install python-pptx")
        
        logger.info(f"ğŸ“Š æ­£åœ¨è§£æ PowerPoint: {path.name}")
        
        prs = Presentation(str(path))
        texts = []
        
        for slide_num, slide in enumerate(prs.slides, 1):
            slide_texts = [f"=== å¹»ç¯ç‰‡ {slide_num} ==="]
            
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text:
                    slide_texts.append(shape.text)
            
            if len(slide_texts) > 1:
                texts.append("\n".join(slide_texts))
        
        return "\n\n".join(texts)
    
    def _parse_docx(self, path: Path) -> str:
        """è§£æ Word æ–‡ä»¶ - ä½¿ç”¨ python-docx"""
        try:
            from docx import Document
        except ImportError:
            raise ImportError("è¯·å®‰è£… python-docx: pip install python-docx")
        
        logger.info(f"ğŸ“ æ­£åœ¨è§£æ Word æ–‡æ¡£: {path.name}")
        
        doc = Document(str(path))
        texts = []
        
        # æå–æ®µè½
        for para in doc.paragraphs:
            if para.text.strip():
                texts.append(para.text)
        
        # æå–è¡¨æ ¼
        for table in doc.tables:
            table_texts = []
            for row in table.rows:
                row_text = " | ".join(cell.text.strip() for cell in row.cells)
                if row_text.strip():
                    table_texts.append(row_text)
            if table_texts:
                texts.append("\nè¡¨æ ¼å†…å®¹:\n" + "\n".join(table_texts))
        
        return "\n\n".join(texts)
    
    def _parse_markdown(self, path: Path) -> str:
        """è§£æ Markdown æ–‡ä»¶"""
        logger.info(f"ğŸ“– æ­£åœ¨è§£æ Markdown: {path.name}")
        
        # Markdown ç›´æ¥è¯»å–åŸå§‹æ–‡æœ¬å³å¯
        return path.read_text(encoding='utf-8')
    
    def _parse_text(self, path: Path) -> str:
        """è§£æçº¯æ–‡æœ¬æ–‡ä»¶"""
        logger.info(f"ğŸ“ƒ æ­£åœ¨è§£ææ–‡æœ¬æ–‡ä»¶: {path.name}")
        
        # å°è¯•å¤šç§ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
        
        for encoding in encodings:
            try:
                return path.read_text(encoding=encoding)
            except UnicodeDecodeError:
                continue
        
        # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨ errors='ignore'
        return path.read_text(encoding='utf-8', errors='ignore')
    
    def _extract_text_from_elements(self, text: str) -> str:
        """å…¼å®¹æ¥å£ - ç›´æ¥è¿”å›æ–‡æœ¬"""
        return text
    
    def _serialize_elements(self, text: str) -> List[Dict[str, Any]]:
        """åºåˆ—åŒ–ä¸ºå…ƒç´ åˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç®€å•æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        return [
            {
                "type": "Paragraph",
                "text": para,
                "category": "text"
            }
            for para in paragraphs
        ]
    
    def get_file_info(self, file_path: str) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯"""
        path = Path(file_path)
        
        if not path.exists():
            return {
                "exists": False,
                "error": "æ–‡ä»¶ä¸å­˜åœ¨"
            }
        
        file_ext = path.suffix.lower()
        mime_type, _ = mimetypes.guess_type(str(path))
        
        return {
            "exists": True,
            "name": path.name,
            "extension": file_ext,
            "mime_type": mime_type,
            "size_bytes": path.stat().st_size,
            "size_kb": path.stat().st_size / 1024,
            "size_mb": path.stat().st_size / (1024 * 1024),
            "supported": file_ext in self.SUPPORTED_EXTENSIONS
        }


def parse_file_for_llm(
    file_path: str,
    max_chars: int = 30000,
    extract_tables: bool = True
) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šè§£ææ–‡ä»¶å¹¶è¿”å›é€‚åˆ LLM çš„æ–‡æœ¬
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        max_chars: æœ€å¤§å­—ç¬¦æ•°ï¼ˆè€ƒè™‘ LLM context é™åˆ¶ï¼‰
        extract_tables: æ˜¯å¦æå–è¡¨æ ¼
    
    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹
    
    Raises:
        Exception: è§£æå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    parser = FileParser()
    result = parser.parse_file(
        file_path=file_path,
        max_chars=max_chars,
        extract_tables=extract_tables
    )
    
    if not result["success"]:
        raise Exception(f"æ–‡ä»¶è§£æå¤±è´¥: {result.get('error', 'Unknown error')}")
    
    return result["text"]


def analyze_file_with_llm(
    file_path: str,
    analysis_prompt: Optional[str] = None,
    max_file_chars: int = 20000
) -> Dict[str, Any]:
    """
    è§£ææ–‡ä»¶å¹¶ä½¿ç”¨ LLM åˆ†æå†…å®¹
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        analysis_prompt: è‡ªå®šä¹‰åˆ†ææç¤ºï¼ˆå¯é€‰ï¼‰
        max_file_chars: æ–‡ä»¶å†…å®¹æœ€å¤§å­—ç¬¦æ•°
    
    Returns:
        LLM åˆ†æç»“æœ
    """
    from openai import OpenAI
    import os
    
    # æ£€æŸ¥ API Key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return {
            "success": False,
            "error": "OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®"
        }
    
    # è§£ææ–‡ä»¶
    parser = FileParser()
    parse_result = parser.parse_file(
        file_path=file_path,
        max_chars=max_file_chars,
        extract_tables=True
    )
    
    if not parse_result["success"]:
        return {
            "success": False,
            "error": f"æ–‡ä»¶è§£æå¤±è´¥: {parse_result.get('error')}"
        }
    
    file_text = parse_result["text"]
    file_name = parse_result["file_name"]
    
    # é»˜è®¤åˆ†ææç¤º
    if not analysis_prompt:
        analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

æ–‡ä»¶å: {file_name}

æ–‡æ¡£å†…å®¹:
{file_text}

è¯·æå–ï¼š
1. ä¸»è¦å†…å®¹æ‘˜è¦ï¼ˆ3-5å¥è¯ï¼‰
2. å…³é”®è¦ç‚¹å’Œé‡è¦ä¿¡æ¯
3. å¦‚æœæ˜¯è¯¾ç¨‹ä½œä¸šæ–‡æ¡£ï¼Œè¯·æå–ï¼š
   - æˆªæ­¢æ—¥æœŸ
   - ä½œä¸šè¦æ±‚
   - è¯„åˆ†æ ‡å‡†
   - æ³¨æ„äº‹é¡¹
4. å…¶ä»–é‡è¦ä¿¡æ¯

ä»¥ç»“æ„åŒ–çš„æ–¹å¼è¿”å›ã€‚
"""
    else:
        # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºï¼Œä½†ç¡®ä¿åŒ…å«æ–‡ä»¶å†…å®¹
        analysis_prompt = f"""
æ–‡ä»¶å: {file_name}

æ–‡æ¡£å†…å®¹:
{file_text}

{analysis_prompt}
"""
    
    try:
        # è°ƒç”¨ LLM
        base_url = os.getenv("OPENAI_BASE_URL")
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        
        if base_url:
            client = OpenAI(api_key=api_key, base_url=base_url)
        else:
            client = OpenAI(api_key=api_key)
        
        logger.info(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨ {model} åˆ†ææ–‡æ¡£...")
        
        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»å„ç±»æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯å’Œç»“æ„åŒ–æ•°æ®ã€‚"
                },
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ],
            temperature=0.3,
            max_tokens=2000
        )
        
        analysis_result = response.choices[0].message.content
        
        return {
            "success": True,
            "file_name": file_name,
            "file_type": parse_result["file_type"],
            "char_count": parse_result["char_count"],
            "element_count": parse_result["element_count"],
            "analysis": analysis_result,
            "model": model,
            "tokens_used": {
                "prompt": response.usage.prompt_tokens,
                "completion": response.usage.completion_tokens,
                "total": response.usage.total_tokens
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ LLM åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error": f"LLM åˆ†æå¤±è´¥: {str(e)}",
            "file_text": file_text  # è¿”å›æ–‡æœ¬ä¾›åç»­å¤„ç†
        }


# ä¾¿æ·å‡½æ•°é›†åˆ

def extract_text_from_pdf(pdf_path: str, max_chars: Optional[int] = None) -> str:
    """ä» PDF æå–æ–‡æœ¬"""
    return parse_file_for_llm(pdf_path, max_chars=max_chars or 50000)


def extract_text_from_docx(docx_path: str, max_chars: Optional[int] = None) -> str:
    """ä» Word æ–‡æ¡£æå–æ–‡æœ¬"""
    return parse_file_for_llm(docx_path, max_chars=max_chars or 50000)


def extract_text_from_pptx(pptx_path: str, max_chars: Optional[int] = None) -> str:
    """ä» PowerPoint æå–æ–‡æœ¬"""
    return parse_file_for_llm(pptx_path, max_chars=max_chars or 50000)


def batch_parse_files(file_paths: List[str]) -> List[Dict[str, Any]]:
    """æ‰¹é‡è§£æå¤šä¸ªæ–‡ä»¶"""
    parser = FileParser()
    results = []
    
    for file_path in file_paths:
        result = parser.parse_file(file_path)
        results.append({
            "file": file_path,
            **result
        })
    
    return results


if __name__ == "__main__":
    """æµ‹è¯•ä»£ç """
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python file_parser.py <file_path> [--llm]")
        print("ç¤ºä¾‹: python file_parser.py test.pdf")
        print("ç¤ºä¾‹: python file_parser.py test.docx --llm")
        sys.exit(1)
    
    file_path = sys.argv[1]
    enable_llm = "--llm" in sys.argv
    
    parser = FileParser()
    
    # è·å–æ–‡ä»¶ä¿¡æ¯
    info = parser.get_file_info(file_path)
    print(f"\nğŸ“ æ–‡ä»¶ä¿¡æ¯:")
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    # è§£ææ–‡ä»¶
    result = parser.parse_file(file_path, max_chars=30000)
    
    if result["success"]:
        print(f"\nâœ… è§£ææˆåŠŸï¼")
        print(f"  - å…ƒç´ æ•°é‡: {result['element_count']}")
        print(f"  - å­—ç¬¦æ•°é‡: {result['char_count']}")
        print(f"\nğŸ“„ æ–‡æœ¬é¢„è§ˆ (å‰500å­—ç¬¦):")
        print("-" * 60)
        print(result["text"][:500])
        print("-" * 60)
        
        # LLM åˆ†æ
        if enable_llm:
            print("\nğŸ¤– æ­£åœ¨è°ƒç”¨ LLM åˆ†ææ–‡æ¡£...")
            llm_result = analyze_file_with_llm(file_path)
            
            if llm_result["success"]:
                print(f"\nâœ… LLM åˆ†æå®Œæˆ (ç”¨äº† {llm_result['tokens_used']['total']} tokens)")
                print("\nğŸ“Š åˆ†æç»“æœ:")
                print("-" * 60)
                print(llm_result["analysis"])
                print("-" * 60)
            else:
                print(f"\nâŒ LLM åˆ†æå¤±è´¥: {llm_result['error']}")
    else:
        print(f"\nâŒ è§£æå¤±è´¥: {result['error']}")

