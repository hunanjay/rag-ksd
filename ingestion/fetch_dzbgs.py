import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import hashlib
import os
import sys
import psycopg2
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from ingestion.splitter import split_text

load_dotenv()

# è¦çˆ¬çš„åˆ—è¡¨
urls = [
    "https://www.jxstnu.edu.cn/info/1781/70441.htm"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0",
    "Accept": "text/html,application/xhtml+xml"
}

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": os.getenv("POSTGRES_HOST", "localhost"),
    "port": os.getenv("POSTGRES_PORT", "5432"),
    "database": os.getenv("POSTGRES_DB", "rag"),
    "user": os.getenv("POSTGRES_USER", "rag"),
    "password": os.getenv("POSTGRES_PASSWORD", "rag"),
}

# Embedding é…ç½®
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-large")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")


def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        return conn
    except psycopg2.OperationalError as e:
        print(f"\nâŒ æ•°æ®åº“è¿æ¥å¤±è´¥!")
        print(f"  é…ç½®ä¿¡æ¯:")
        print(f"    ä¸»æœº: {DB_CONFIG['host']}:{DB_CONFIG['port']}")
        print(f"    æ•°æ®åº“: {DB_CONFIG['database']}")
        print(f"    ç”¨æˆ·: {DB_CONFIG['user']}")
        print(f"  é”™è¯¯è¯¦æƒ…: {str(e)}")
        print(f"\nğŸ’¡ è¯·æ£€æŸ¥:")
        print(f"  1. PostgreSQL æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ")
        print(f"  2. .env æ–‡ä»¶ä¸­çš„æ•°æ®åº“é…ç½®æ˜¯å¦æ­£ç¡®")
        print(f"  3. æ•°æ®åº“ '{DB_CONFIG['database']}' æ˜¯å¦å­˜åœ¨")
        print(f"  4. ç”¨æˆ· '{DB_CONFIG['user']}' æ˜¯å¦æœ‰è®¿é—®æƒé™")
        print(f"  5. é˜²ç«å¢™æ˜¯å¦å…è®¸è¿æ¥åˆ°ç«¯å£ {DB_CONFIG['port']}")
        raise
    except Exception as e:
        print(f"\nâŒ æ•°æ®åº“è¿æ¥æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        raise


def calculate_hash(text: str) -> str:
    """è®¡ç®—æ–‡æœ¬çš„ SHA256 å“ˆå¸Œå€¼"""
    return hashlib.sha256(text.encode('utf-8')).hexdigest()


def get_embeddings_model():
    """è·å– embedding æ¨¡å‹ï¼ŒæŒ‡å®š 1024 ç»´åº¦"""
    kwargs = {
        "model": EMBEDDING_MODEL,
        "openai_api_key": OPENAI_API_KEY,
        "dimensions": 1024,  # æŒ‡å®šç”Ÿæˆ 1024 ç»´åº¦çš„å‘é‡ï¼ŒåŒ¹é…æ•°æ®åº“ schema
    }
    if OPENAI_BASE_URL:
        kwargs["openai_api_base"] = OPENAI_BASE_URL
    return OpenAIEmbeddings(**kwargs)


def fetch(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.encoding = resp.apparent_encoding
        return resp.text
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {url} => {e}")
        return ""


def parse_content(html, base_url):
    soup = BeautifulSoup(html, "lxml")

    # æ ‡é¢˜
    title = soup.find("title").get_text(strip=True) if soup.find("title") else ""

    # å°½é‡æ‰¾æ­£æ–‡åŒºåŸŸ
    main = soup.find("div", class_="news_content") or soup.find("div", id="content") or soup

    # è·å–çº¯æ–‡æœ¬
    text = main.get_text("\n", strip=True)

    # è·å–å›¾ç‰‡
    imgs = []
    for img in main.find_all("img"):
        src = img.get("src")
        if src:
            imgs.append(urljoin(base_url, src))

    return {
        "title": title,
        "text": text,
        "images": imgs
    }


def save_document_to_db(url: str, title: str, content: str, content_hash: str) -> int:
    """
    ä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“ï¼Œè¿”å› document_id
    å¦‚æœ content_hash å·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰æ–‡æ¡£çš„ ID
    """
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒ hash çš„æ–‡æ¡£
            cur.execute(
                "SELECT id FROM public.document WHERE content_hash = %s",
                (content_hash,)
            )
            existing = cur.fetchone()
            
            if existing:
                doc_id = existing[0]
                print(f"  æ–‡æ¡£å·²å­˜åœ¨ (hash ç›¸åŒ)ï¼ŒID: {doc_id}")
                return doc_id
            
            # æ’å…¥æ–°æ–‡æ¡£
            cur.execute(
                """
                INSERT INTO public.document (source_url, title, content, content_hash)
                VALUES (%s, %s, %s, %s)
                RETURNING id
                """,
                (url, title, content, content_hash)
            )
            doc_id = cur.fetchone()[0]
            conn.commit()
            print(f"  æ–‡æ¡£å·²ä¿å­˜ï¼ŒID: {doc_id}")
            return doc_id
    finally:
        conn.close()


def save_chunks_to_db(document_id: int, chunks: list, embeddings_model):
    """
    ä¿å­˜ chunks åˆ°æ•°æ®åº“ï¼Œå¹¶ç”Ÿæˆ embeddings
    """
    if not chunks:
        print("  æ²¡æœ‰ chunks éœ€è¦ä¿å­˜")
        return
    
    conn = get_db_connection()
    try:
        # ç”Ÿæˆ embeddings
        print(f"  æ­£åœ¨ç”Ÿæˆ {len(chunks)} ä¸ª chunks çš„ embeddings...")
        embeddings = embeddings_model.embed_documents(chunks)
        
        with conn.cursor() as cur:
            # é€ä¸ªæ’å…¥ chunksï¼ˆå› ä¸º vector ç±»å‹éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
            for idx, (chunk_text, embedding) in enumerate(zip(chunks, embeddings)):
                # å°† embedding åˆ—è¡¨è½¬æ¢ä¸º PostgreSQL vector æ ¼å¼å­—ç¬¦ä¸²
                embedding_str = "[" + ",".join(map(str, embedding)) + "]"
                cur.execute(
                    """
                    INSERT INTO public.document_chunk (document_id, chunk_index, content, embedding)
                    VALUES (%s, %s, %s, %s::vector)
                    """,
                    (document_id, idx, chunk_text, embedding_str)
                )
            conn.commit()
            print(f"  {len(chunks)} ä¸ª chunks å·²ä¿å­˜å¹¶ç”Ÿæˆ embeddings")
    finally:
        conn.close()


def process_and_save(data: dict, embeddings_model):
    """
    å¤„ç†å•ä¸ªæ–‡æ¡£ï¼šè®¡ç®— hashã€ä¿å­˜åˆ°æ•°æ®åº“ã€åˆ’åˆ† chunksã€ç”Ÿæˆ embeddings
    """
    url = data["url"]
    title = data["title"]
    text = data["text"]
    
    if not text or not text.strip():
        print(f"è·³è¿‡ç©ºå†…å®¹: {url}")
        return
    
    # è®¡ç®— content_hash
    content_hash = calculate_hash(text)
    print(f"å¤„ç†æ–‡æ¡£: {title}")
    print(f"  URL: {url}")
    print(f"  Hash: {content_hash[:16]}...")
    
    # ä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“
    doc_id = save_document_to_db(url, title, text, content_hash)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° chunksï¼ˆå¦‚æœæ–‡æ¡£å·²å­˜åœ¨ä¸” hash ç›¸åŒï¼Œå¯èƒ½ä¸éœ€è¦é‡æ–°ç”Ÿæˆï¼‰
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            cur.execute(
                "SELECT COUNT(*) FROM public.document_chunk WHERE document_id = %s",
                (doc_id,)
            )
            chunk_count = cur.fetchone()[0]
            
            if chunk_count > 0:
                print(f"  æ–‡æ¡£å·²æœ‰ {chunk_count} ä¸ª chunksï¼Œè·³è¿‡é‡æ–°ç”Ÿæˆ")
                return
    finally:
        conn.close()
    
    # åˆ’åˆ† chunks
    print(f"  æ­£åœ¨åˆ’åˆ† chunks...")
    chunks = split_text(text, chunk_size=500, chunk_overlap=100)
    print(f"  åˆ’åˆ†ä¸º {len(chunks)} ä¸ª chunks")
    
    # ä¿å­˜ chunks å¹¶ç”Ÿæˆ embeddings
    save_chunks_to_db(doc_id, chunks, embeddings_model)


def spider():
    """çˆ¬å–æ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
    results = []
    for url in urls:
        html = fetch(url)
        if not html:
            continue

        data = parse_content(html, url)
        data["url"] = url
        results.append(data)

        print(f"æŠ“å–æˆåŠŸ: {url}")
        time.sleep(1)

    return results


if __name__ == "__main__":
    # è·å– embedding æ¨¡å‹
    print("åˆå§‹åŒ– embedding æ¨¡å‹...")
    embeddings_model = get_embeddings_model()
    
    # çˆ¬å–æ•°æ®
    print("å¼€å§‹çˆ¬å–æ•°æ®...")
    data_list = spider()
    
    # å¤„ç†å¹¶ä¿å­˜æ¯ä¸ªæ–‡æ¡£
    print("\nå¼€å§‹ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
    for data in data_list:
        try:
            process_and_save(data, embeddings_model)
            print()
        except Exception as e:
            print(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}\n")
            import traceback
            traceback.print_exc()
    
    print("å®Œæˆï¼")
